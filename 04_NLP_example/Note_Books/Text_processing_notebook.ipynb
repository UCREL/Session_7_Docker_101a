{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UCREL/IAA-Oracle-ULTEC/blob/main/Text_processing_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a16eaa79",
      "metadata": {
        "id": "a16eaa79"
      },
      "source": [
        "#  Text Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb715b6a",
      "metadata": {
        "id": "eb715b6a"
      },
      "source": [
        "\n",
        "This notebook provides a detailed explanation of a Python script for text processing, highlighting the use of `spaCy`, asynchronous programming with `asyncio`, and a specific focus on the `pymusas_rule_based_tagger` for semantic analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `os`, `json`, `Path`: These modules are used for interacting with the file system, reading, and writing JSON data.\n",
        "- `spaCy`: A powerful library for natural language processing (NLP).\n",
        "- `NamedEntityExtractor`: A custom class ( defined in the Named entity Notebook) for extracting named entities from text.\n",
        "- `asyncio`: Library for asynchronous HTTP requests and asynchronous programming.\n"
      ],
      "metadata": {
        "id": "IowMRUx_g7Ij"
      },
      "id": "IowMRUx_g7Ij"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a933300",
      "metadata": {
        "id": "4a933300"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from Named_entity_extractor import NamedEntityExtractor\n",
        "import asyncio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1991f2d",
      "metadata": {
        "id": "e1991f2d"
      },
      "source": [
        "\n",
        "## spaCy + Pymusas Model Configuration\n",
        "\n",
        "The script begins by loading the `en_core_web_sm` spaCy model, excluding the parser to enhance performance. It then loads an additional pipeline, `en_dual_none_contextual`, and adds the `pymusas_rule_based_tagger` from this pipeline to the main NLP model. This tagger is used for semantic analysis, leveraging the Pymusas semantic analyser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b27dc51",
      "metadata": {
        "id": "4b27dc51"
      },
      "outputs": [],
      "source": [
        "NLP_MODEL = spacy.load('en_core_web_sm', exclude=['parser'])\n",
        "english_tagger_pipeline = spacy.load('en_dual_none_contextual')\n",
        "NLP_MODEL.add_pipe('pymusas_rule_based_tagger', source=english_tagger_pipeline)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aadb99b0",
      "metadata": {
        "id": "aadb99b0"
      },
      "source": [
        "\n",
        "## TextProcessor Class\n",
        "\n",
        "The `TextProcessor` class is designed for processing text files using the configured NLP model. It includes methods for loading JSON data, processing text to extract features and named entities, and saving processed data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Asynchronously processes a file by loading its data, processing the text, and saving the results.\n",
        "\n",
        "#### Data Processing Methods\n",
        "Includes methods for:\n",
        "- Loading JSON data (`load_json_data`),\n",
        "- Processing the text data asynchronously (`process_data`),\n",
        "- Saving the processed data (`save_processed_data` and `write_to_file`).\n",
        "\n",
        "`process_data` tokenizes text, extracts named entities, and enriches tokens with additional information like POS tags, USAS tags, and geographical coordinates if available.\n",
        "\n",
        "The saving process is split into two methods:\n",
        "- `save_processed_data` manages the data flow, ensuring data is written correctly for each page,\n",
        "- `write_to_file` actually writes the data to a JSON file.\n"
      ],
      "metadata": {
        "id": "dNlkqAPdhqJX"
      },
      "id": "dNlkqAPdhqJX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c03aef",
      "metadata": {
        "id": "08c03aef"
      },
      "outputs": [],
      "source": [
        "class TextProcessor:\n",
        "    def __init__(self, input_path, NLP_MODEL):\n",
        "        self.input_path = Path(input_path)\n",
        "        self.output_path = Path(output_folder_path)\n",
        "        self.nlp = NLP_MODEL\n",
        "        self.nee = NamedEntityExtractor(NLP_MODEL)\n",
        "\n",
        "    async def process_file(self, file_name):\n",
        "        file_path = self.input_path / file_name\n",
        "        print(f'Processing file {file_path}')\n",
        "\n",
        "        data = self.load_json_data(file_path)\n",
        "        if data is not None:\n",
        "            corrected_data = await self.process_data(data)\n",
        "            self.save_processed_data(file_path, corrected_data)\n",
        "        else:\n",
        "            print(f\"Failed to load data from {file_path}\")\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def load_json_data(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r') as file:\n",
        "                return json.load(file)\n",
        "        except IOError:\n",
        "            print(f\"Error: File {file_path} does not appear to exist.\")\n",
        "            return None\n",
        "\n",
        "    async def process_data(self, data):\n",
        "        \"\"\"\n",
        "            Processes textual data from a list of pages. Each page is tokenized into words using a NLP pipeline,\n",
        "            and for each word, linguistic features are extracted.\n",
        "\n",
        "            Parameters:\n",
        "            - data (list of tuples): Each tuple contains a page number and its corresponding text.\n",
        "                - page_number (int): The number of the page. This is used for tracking and referencing the source of each tokenized word.\n",
        "                - text (str): The textual content of the page. This text is processed and tokenized into individual words or tokens.\n",
        "                - start_char and end_char to be used later in KWIC and collocations\n",
        "\n",
        "            Returns:\n",
        "            - list of dicts: Each dictionary contains information about a token, including its text, lemma,\n",
        "            POS tag, semantic tag, page number, and character positions.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        processed_data = []\n",
        "        for page in data:\n",
        "            page_number, text = page\n",
        "            doc = self.nlp(text)\n",
        "            ne_data = await self.nee.process_text(text)  # Process text for named entities\n",
        "                        # Extract latitude and longitude if they exist\n",
        "\n",
        "            for token in doc:\n",
        "                # Find the named entity in ne_data that corresponds to the token\n",
        "                ne_info = next((ne for ne in ne_data if ne[0] == token.text), None)\n",
        "                latitude = None\n",
        "                longitude = None\n",
        "                if ne_info and len(ne_info) > 2 and isinstance(ne_info[2], dict):\n",
        "                    latitude_str = ne_info[2].get('latitude')\n",
        "                    longitude_str = ne_info[2].get('longitude')\n",
        "\n",
        "                    if latitude_str is not None and longitude_str is not None:\n",
        "                        try:\n",
        "                            latitude = float(latitude_str)\n",
        "                            longitude = float(longitude_str)\n",
        "                        except ValueError:\n",
        "                            # Handle the case where latitude or longitude is not a valid number\n",
        "                            print(f\"Invalid latitude or longitude value for {token.text}: latitude={latitude_str}, longitude={longitude_str}\")\n",
        "\n",
        "                token_data = {\n",
        "                    'text': token.text,\n",
        "                    'lemma': token.lemma_,\n",
        "                    'POS': token.pos_,\n",
        "                    'USAS_tags': token._.pymusas_tags,\n",
        "                    'page_id': page_number,\n",
        "                    'start_char': token.idx,  # Character start position\n",
        "                    'end_char': token.idx + len(token),  # Character end position\n",
        "                    'NE': ne_info,  # Add named entity information\n",
        "                    'latitude': latitude,  # Add latitude\n",
        "                    'longitude': longitude  # Add longitude\n",
        "                }\n",
        "                processed_data.append(token_data)\n",
        "        return processed_data\n",
        "\n",
        "    def save_processed_data(self, original_file_path, data):\n",
        "        current_page_id = None\n",
        "        page_data = []\n",
        "\n",
        "        for item in data:\n",
        "            if current_page_id is None:\n",
        "                current_page_id = item['page_id']\n",
        "\n",
        "            if item['page_id'] != current_page_id:\n",
        "                # Save the data accumulated so far for the previous page\n",
        "                self.write_to_file(original_file_path, page_data, current_page_id)\n",
        "                page_data = []  # Reset page_data for the new page\n",
        "                current_page_id = item['page_id']\n",
        "\n",
        "            page_data.append(item)\n",
        "\n",
        "        # Don't forget to save the last page's data\n",
        "        if page_data:\n",
        "            self.write_to_file(original_file_path, page_data, current_page_id)\n",
        "\n",
        "    def write_to_file(self, original_file_path, page_data, page_id):\n",
        "        output_file = self.output_path / f\"{original_file_path.stem}_page_{page_id}.json\"\n",
        "        with open(output_file, 'w') as file:\n",
        "            json.dump(page_data, file, indent=4)\n",
        "        print(f'Saved file {output_file}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ae570cc",
      "metadata": {
        "id": "6ae570cc"
      },
      "source": [
        "\n",
        "### `__init__` Method\n",
        "\n",
        "Initializes the TextProcessor with input and output paths, the NLP model, and a named entity extractor instance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36a1d681",
      "metadata": {
        "id": "36a1d681"
      },
      "source": [
        "\n",
        "### `process_file` Method\n",
        "\n",
        "Asynchronously processes a single file, loading its data, processing it through `process_data`, and saving the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e5a87b8",
      "metadata": {
        "id": "9e5a87b8"
      },
      "source": [
        "\n",
        "### `load_json_data` Static Method\n",
        "\n",
        "Loads JSON data from a specified file path, handling IO errors gracefully."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449513a6",
      "metadata": {
        "id": "449513a6"
      },
      "source": [
        "\n",
        "### `process_data` Method\n",
        "\n",
        "Processes textual data using the NLP model to tokenize text, extract features, and enrich tokens with semantic tags from the `pymusas_rule_based_tagger`, alongside extracting named entities and geographical coordinates if available."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd26469",
      "metadata": {
        "id": "4bd26469"
      },
      "source": [
        "\n",
        "### `save_processed_data` and `write_to_file` Methods\n",
        "\n",
        "Handle the organization and saving of processed data into JSON files, ensuring data is correctly segmented and saved per page."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Asynchronous Main Function\n",
        "This outlines the creation of an asynchronous main function responsible for initializing the text processor and handling the processing of the specified file.\n",
        "\n",
        "- Utilizes `asyncio.run()` for invoking the main function, aligning with asynchronous programming conventions.\n"
      ],
      "metadata": {
        "id": "6J_L0OJiiZoc"
      },
      "id": "6J_L0OJiiZoc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff406820",
      "metadata": {
        "id": "ff406820"
      },
      "outputs": [],
      "source": [
        "input_folder_path = os.getenv('INPUT_FOLDER_PATH', '/path/to/input')\n",
        "output_folder_path = os.getenv('OUTPUT_FOLDER_PATH', '/path/to/output')\n",
        "file_name = os.getenv('FILE_NAME')\n",
        "\n",
        "async def main():\n",
        "    processor = TextProcessor(input_folder_path, NLP_MODEL)\n",
        "    await processor.process_file(file_name)\n",
        "\n",
        "asyncio.run(main())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}