{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UCREL/IAA-Oracle-ULTEC/blob/main/Named_entity_extractor_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Extraction with Plant Names"
      ],
      "metadata": {
        "id": "XLUoF1FcpdXC"
      },
      "id": "XLUoF1FcpdXC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This notebook provides a detailed explanation of the `NamedEntityExtractor` class for named entity extraction, emphasizing the addition of plant names to the recognized entities. The code is adapted and modified from an existing demo at https://github.com/SpaceTimeNarratives/demo.\n"
      ],
      "metadata": {
        "id": "CnT_e3cgpjBC"
      },
      "id": "CnT_e3cgpjBC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "The `NamedEntityExtractor` is initialized with a spaCy NLP model and sets up an entity ruler for custom pattern matching. The class has several key functions, including setting up entity patterns, merging entities, geocoding, and more. Each of these functions plays a crucial role in identifying and processing entities within the text.\n"
      ],
      "metadata": {
        "id": "dKmOk7_uwnrx"
      },
      "id": "dKmOk7_uwnrx"
    },
    {
      "cell_type": "markdown",
      "id": "76b87cce",
      "metadata": {
        "id": "76b87cce"
      },
      "source": [
        "# NamedEntityExtractor Class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f40312",
      "metadata": {
        "id": "23f40312"
      },
      "source": [
        "`NamedEntityExtractor` class,  aims to elucidate how the class operates, especially its use of spaCy for natural language processing and its approach to recognizing and extracting named entities, with particular attention to plant names.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NamedEntityExtractor:\n",
        "    def __init__(self, nlp_model):\n",
        "        self.nlp = nlp_model\n",
        "        self.nlp.add_pipe(\"sentencizer\")\n",
        "        self.ruler = self.nlp.add_pipe(\"entity_ruler\", before='ner')\n",
        "        self.setup_entity_patterns()\n",
        "        self.combine = lambda x, y: (x[0], x[1], x[2]+' '+y[2], x[3])\n",
        "        self.geolocation_tags = ['GEO', 'PLNAME', 'GPE']\n",
        "        self.geocode_cache = {}"
      ],
      "metadata": {
        "id": "MWgwPB6fwztk"
      },
      "id": "MWgwPB6fwztk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6f85b739",
      "metadata": {
        "id": "6f85b739"
      },
      "source": [
        "## setup_entity_patterns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Initialization and Entity Ruler Setup\n",
        "\n",
        "The class is initialized with a spaCy NLP model. A sentencizer is added to split the text into sentences, which helps in understanding the context better. An entity ruler is then added for pattern matching, which is crucial for recognizing custom entities like plant names.\n"
      ],
      "metadata": {
        "id": "QydEu4WIwxSH"
      },
      "id": "QydEu4WIwxSH"
    },
    {
      "cell_type": "markdown",
      "id": "e9dd9fea",
      "metadata": {
        "id": "e9dd9fea"
      },
      "source": [
        "\n",
        "**Purpose:** Initializes entity patterns for spaCy's EntityRuler. It loads lists of terms (e.g., plant names, locations) from external files and creates matcher patterns.\n",
        "\n",
        "**Inputs:** None directly; utilizes external text files as sources for terms.\n",
        "\n",
        "**Outputs:** Updates the EntityRuler in the spaCy pipeline with custom patterns for named entity recognition.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_entity_patterns(self):\n",
        "        # Get the list of placenames and geonouns\n",
        "        place_names = [name.strip().title().replace(\"'S\", \"'s\") for name in open('resources/LD_placenames.txt').readlines()] #read and convert to title case\n",
        "        place_names += [name.upper() for name in place_names] #retain the upper case versions\n",
        "        geonouns = self.get_inflections([noun.strip() for noun in open('resources/geo_feature_nouns.txt').readlines()])\n",
        "\n",
        "        # Get the locative adverbs\n",
        "        loc_advs = [l.split()[0] for l in open('resources/locative_adverbs.txt').readlines()]\n",
        "        sp_prep  = [l.strip() for l in open('resources/spatial_prepositions.txt').readlines()\n",
        "                                                                    if len(l.strip())>2]\n",
        "        # Get distances\n",
        "        distances = [l.strip() for l in open('resources/distances.txt').readlines()]\n",
        "\n",
        "        # Get dates\n",
        "        dates     = [l.strip() for l in open('resources/dates.txt').readlines()]\n",
        "\n",
        "        # Get times\n",
        "        times     = [l.strip() for l in open('resources/times.txt').readlines()]\n",
        "\n",
        "        # Get events\n",
        "        events    = [l.strip() for l in open('resources/events.txt').readlines()]\n",
        "\n",
        "        # Get Plant- names this is new adding the plant names list to the NES\n",
        "        pnames = [l.strip() for l in open('resources/Plant_list.txt').readlines()]\n",
        "\n",
        "        # Get the list of positive and negative words from the sentiment lexicon\n",
        "        pos_words = [w.strip() for w in open('resources/positive-words.txt','r', encoding='latin-1').readlines()[35:]]\n",
        "        neg_words = [w.strip() for w in open('resources/negative-words.txt','r', encoding='latin-1').readlines()[35:]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Define the patterns for the EntityRuler by labelling all the names with the tag PLNAME\n",
        "        patterns = [{\"label\": \"PLANT\", \"pattern\": word} for word in pnames]\n",
        "        patterns +=  [{\"label\": \"PLNAME\",  \"pattern\": plname} for plname in set(place_names)]\n",
        "        patterns += [{\"label\": \"GEONOUN\", \"pattern\": noun} for noun in geonouns]\n",
        "        patterns += [{\"label\": \"+EMOTION\", \"pattern\": word} for word in pos_words]\n",
        "        patterns += [{\"label\": \"-EMOTION\", \"pattern\": word} for word in neg_words]\n",
        "        patterns += [{\"label\": \"EVENT\",   \"pattern\": word} for word in events]\n",
        "        patterns += [{\"label\": \"DATE\", \"pattern\": word} for word in dates]\n",
        "        patterns += [{\"label\": \"TIME\", \"pattern\": word} for word in times]\n",
        "        patterns += [{\"label\": \"DISTANCE\", \"pattern\": word} for word in distances]\n",
        "        patterns += [{\"label\": \"LOCADV\", \"pattern\": word} for word in loc_advs]\n",
        "        patterns += [{\"label\": \"SP-PREP\", \"pattern\": word} for word in sp_prep]\n",
        "\n",
        "\n",
        "        self.ruler.add_patterns(patterns)\n",
        "\n"
      ],
      "metadata": {
        "id": "DBcoJ-1axlMA"
      },
      "id": "DBcoJ-1axlMA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d3f8da08",
      "metadata": {
        "id": "d3f8da08"
      },
      "source": [
        "## get_inflections"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e883323",
      "metadata": {
        "id": "3e883323"
      },
      "source": [
        "\n",
        "**Purpose:** Generates inflected forms of nouns, aiding in the comprehensive matching of entities regardless of their grammatical number.\n",
        "\n",
        "**Inputs:** List of base nouns.\n",
        "\n",
        "**Outputs:** Extended list including inflected forms of the input nouns.\n",
        "    \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a53e97",
      "metadata": {
        "id": "c3a53e97"
      },
      "outputs": [],
      "source": [
        "# Get inflections and lemmas of geo nouns\n",
        "def get_inflections(self,names_list):\n",
        "        gf_names_inflected = []\n",
        "        for w in names_list:\n",
        "            gf_names_inflected.append(w)\n",
        "            gf_names_inflected.extend(list(getInflection(w.strip(), tag='NNS', inflect_oov=False)))\n",
        "            gf_names_inflected.extend(list(getLemma(w.strip(), 'NOUN', lemmatize_oov=False)))\n",
        "        return list(set(gf_names_inflected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63beb87f",
      "metadata": {
        "id": "63beb87f"
      },
      "source": [
        "## combine_multi_tokens Function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "163860b7",
      "metadata": {
        "id": "163860b7"
      },
      "source": [
        "**Purpose:** Combines adjacent tokens into a single entity when they are part of the same named entity, improving entity recognition accuracy.\n",
        "\n",
        "**Inputs:** Sequence of tokens identified as potential parts of named entities.\n",
        "\n",
        "**Outputs:** List of combined entities where applicable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c33c8547",
      "metadata": {
        "id": "c33c8547"
      },
      "outputs": [],
      "source": [
        "def combine_multi_tokens(self,a_list):\n",
        "        new_list = [a_list.pop()]\n",
        "        while a_list:\n",
        "            last = a_list.pop()\n",
        "            if new_list[-1][0] - last[0] == 1:\n",
        "                new_list.append(self.combine(last, new_list.pop()))\n",
        "            else:\n",
        "                new_list.append(last)\n",
        "        return sorted(new_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2477ca5a",
      "metadata": {
        "id": "2477ca5a"
      },
      "source": [
        "## extract_sem_entities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d5c8ca",
      "metadata": {
        "id": "78d5c8ca"
      },
      "source": [
        "\n",
        "**Purpose:** Extracts semantic entities from processed text, applying custom tags based on predefined patterns.\n",
        "\n",
        "**Inputs:** Processed text (spaCy Doc object) and a list of tag types to extract.\n",
        "\n",
        "**Outputs:** Ordered dictionary of extracted entities with their semantic tags.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e801a42",
      "metadata": {
        "id": "3e801a42"
      },
      "outputs": [],
      "source": [
        "# Generates a dictionary of semantic entities combining adjacent ones\n",
        "def extract_sem_entities(self,processed_text, tag_types):\n",
        "        entities, tokens = {}, [token.text for token in processed_text]\n",
        "        for tag_type in tag_types:\n",
        "            tag_indices = [(i, token.idx, token.text, tag_type) for i, token in enumerate(processed_text)\n",
        "                                if token._.pymusas_tags[0].startswith(tag_type[0])]\n",
        "            if tag_indices:\n",
        "                for i, idx, token, tag in self.combine_multi_tokens(tag_indices):\n",
        "                    entities[idx] = token, tag\n",
        "        return OrderedDict(sorted(entities.items()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa4db8d6",
      "metadata": {
        "id": "aa4db8d6"
      },
      "source": [
        "## merge_entities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed04a5dc",
      "metadata": {
        "id": "ed04a5dc"
      },
      "source": [
        "**Purpose:** Merges adjacent entities of the same type into a single entity to improve entity recognition coherence.\n",
        "\n",
        "**Inputs:** spaCy Doc object with recognized entities.\n",
        "\n",
        "**Outputs:** List of merged entities for further processing or visualization.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa8a1bb3",
      "metadata": {
        "id": "fa8a1bb3"
      },
      "outputs": [],
      "source": [
        "def merge_entities(self, doc):\n",
        "        merged_entities = []\n",
        "        temp_entity = {\"text\": \"\", \"start\": None, \"end\": None, \"label\": None}\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if temp_entity[\"label\"] == ent.label_ and (temp_entity[\"end\"] == ent.start_char or temp_entity[\"end\"] + 1 == ent.start_char):\n",
        "                temp_entity[\"text\"] += \" \" + ent.text\n",
        "                temp_entity[\"end\"] = ent.end_char\n",
        "            else:\n",
        "                if temp_entity[\"text\"]:\n",
        "                    merged_entities.append(temp_entity.copy())\n",
        "                temp_entity = {\"text\": ent.text, \"start\": ent.start_char, \"end\": ent.end_char, \"label\": ent.label_}\n",
        "\n",
        "        if temp_entity[\"text\"]:\n",
        "            merged_entities.append(temp_entity.copy())\n",
        "\n",
        "        return merged_entities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function: geocode"
      ],
      "metadata": {
        "id": "EQL_ZOLwvQT7"
      },
      "id": "EQL_ZOLwvQT7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:** Performs geocoding for place names to obtain latitude and longitude coordinates.\n",
        "\n",
        "**Inputs:** Name of the place to geocode.\n",
        "\n",
        "**Outputs:** Dictionary containing latitude and longitude of the given place name, if found."
      ],
      "metadata": {
        "id": "PsnD_hWJvP_W"
      },
      "id": "PsnD_hWJvP_W"
    },
    {
      "cell_type": "code",
      "source": [
        "async def geocode(self, place_name):\n",
        "        if place_name in self.geocode_cache:\n",
        "            return self.geocode_cache[place_name]\n",
        "\n",
        "        base_url = \"https://nominatim.openstreetmap.org/search\"\n",
        "        params = {'q': place_name, 'format': 'json'}\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.get(base_url, params=params) as response:\n",
        "                if response.status == 200:\n",
        "                    data = await response.json()\n",
        "                    if data:\n",
        "                        result = {'latitude': data[0].get('lat'), 'longitude': data[0].get('lon')}\n",
        "                        self.geocode_cache[place_name] = result\n",
        "                        return result\n",
        "        return {'latitude': None, 'longitude': None}"
      ],
      "metadata": {
        "id": "UzqV1FuEvipI"
      },
      "id": "UzqV1FuEvipI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function: convert_to_iob_format"
      ],
      "metadata": {
        "id": "oygRoqVuvji5"
      },
      "id": "oygRoqVuvji5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:** Converts the list of entities into the IOB (Inside, Outside, Beginning) format, useful for training sequence labeling models.\n",
        "\n",
        "**Inputs:** List of entities and the spaCy Doc object they were extracted from.\n",
        "\n",
        "**Outputs:** List of tokens in the text with their corresponding IOB tags."
      ],
      "metadata": {
        "id": "slEA4qpKvnGy"
      },
      "id": "slEA4qpKvnGy"
    },
    {
      "cell_type": "code",
      "source": [
        "async def convert_to_iob_format(self, merged_entities, doc):\n",
        "        iob_entities = []\n",
        "        for sent in doc.sents:\n",
        "            sent_entities = [e for e in merged_entities if e[\"start\"] >= sent.start_char and e[\"end\"] <= sent.end_char]\n",
        "            for token in sent:\n",
        "                merged_entity = next((e for e in sent_entities if e[\"start\"] <= token.idx < e[\"end\"]), None)\n",
        "                if merged_entity:\n",
        "                    tag_prefix = 'B-' if token.idx == merged_entity[\"start\"] else 'I-'\n",
        "                    base_label = merged_entity[\"label\"].split('-')[-1]\n",
        "                    print('base_label:',base_label )\n",
        "                    if base_label in [\"PLNAME\", \"GEONOUN\",  \"GPE\"]:\n",
        "                        geolocation = await self.geocode(merged_entity[\"text\"])\n",
        "\n",
        "                    else:\n",
        "                        geolocation = None\n",
        "                    iob_entities.append((token.text, tag_prefix + merged_entity[\"label\"], geolocation))\n",
        "                else:\n",
        "                    iob_entities.append((token.text, 'O', None))\n",
        "        return iob_entities"
      ],
      "metadata": {
        "id": "0Dk66b6zv86G"
      },
      "id": "0Dk66b6zv86G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function: process_text\n"
      ],
      "metadata": {
        "id": "CdLbo8sxv-z7"
      },
      "id": "CdLbo8sxv-z7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose:** Main function that processes input text, extracting and annotating named entities based on custom and spaCy's built-in recognizers.\n",
        "\n",
        "**Inputs:** Text to process for named entity extraction.\n",
        "\n",
        "**Outputs:** Processed text with entities annotated, ready for visualization or further analysis."
      ],
      "metadata": {
        "id": "kHME7AX2wDLX"
      },
      "id": "kHME7AX2wDLX"
    },
    {
      "cell_type": "code",
      "source": [
        "async def process_text(self, text):\n",
        "        doc = self.nlp(text)\n",
        "        merged_entities = self.merge_entities(doc)\n",
        "        return await self.convert_to_iob_format(merged_entities, doc)\n"
      ],
      "metadata": {
        "id": "GMmhq6vKwMu0"
      },
      "id": "GMmhq6vKwMu0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1c9d2ef2",
      "metadata": {
        "id": "1c9d2ef2"
      },
      "source": [
        "## visualize_entities Function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10f138cf",
      "metadata": {
        "id": "10f138cf"
      },
      "source": [
        "**Purpose:** Utilizes spaCy's displaCy visualization to render the named entities in the text with custom colors for each entity type.\n",
        "\n",
        "**Inputs:** Text with entities to visualize.\n",
        "\n",
        "**Outputs:** Visualization of text with highlighted entities.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b398e08e",
      "metadata": {
        "id": "b398e08e"
      },
      "outputs": [],
      "source": [
        "def visualize_entities(self, text):\n",
        "        doc = self.nlp(text)\n",
        "        options = {\"ents\": list(BG_COLOR.keys()), \"colors\": BG_COLOR}\n",
        "        displacy.render(doc, style=\"ent\", options=options)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74d2e0f8",
      "metadata": {
        "id": "74d2e0f8"
      },
      "source": [
        "## Class Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba5bd07f",
      "metadata": {
        "id": "ba5bd07f"
      },
      "source": [
        "\n",
        "Finally, here is a complete overview of the `NamedEntityExtractor` class, integrating all the functions discussed. This section aims to provide a holistic view of how the class is structured and how it functions as a whole for named entity extraction.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/UCREL/pymusas-models/releases/download/en_dual_none_contextual-0.3.2/en_dual_none_contextual-0.3.2-py3-none-any.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z1efue7L0XSj",
        "outputId": "ed7eeef2-61e3-4109-db61-d4a11bd332ed"
      },
      "id": "Z1efue7L0XSj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-dual-none-contextual==0.3.2\n",
            "  Downloading https://github.com/UCREL/pymusas-models/releases/download/en_dual_none_contextual-0.3.2/en_dual_none_contextual-0.3.2-py3-none-any.whl (901 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m901.5/901.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from en-dual-none-contextual==0.3.2) (3.6.1)\n",
            "Collecting pymusas<0.4.0,>=0.3.0 (from en-dual-none-contextual==0.3.2)\n",
            "  Downloading pymusas-0.3.0-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from pymusas<0.4.0,>=0.3.0->en-dual-none-contextual==0.3.2) (2.4.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.50.0 in /usr/local/lib/python3.10/dist-packages (from pymusas<0.4.0,>=0.3.0->en-dual-none-contextual==0.3.2) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from pymusas<0.4.0,>=0.3.0->en-dual-none-contextual==0.3.2) (2.31.0)\n",
            "Collecting click<8.1.0 (from pymusas<0.4.0,>=0.3.0->en-dual-none-contextual==0.3.2)\n",
            "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (1.1.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (6.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (1.23.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->pymusas<0.4.0,>=0.3.0->en-dual-none-contextual==0.3.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->pymusas<0.4.0,>=0.3.0->en-dual-none-contextual==0.3.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->pymusas<0.4.0,>=0.3.0->en-dual-none-contextual==0.3.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->pymusas<0.4.0,>=0.3.0->en-dual-none-contextual==0.3.2) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (0.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0,>=3.0->en-dual-none-contextual==0.3.2) (2.1.4)\n",
            "Installing collected packages: click, pymusas, en-dual-none-contextual\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed click-8.0.4 en-dual-none-contextual-0.3.2 pymusas-0.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "click"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lemminflect\n",
        "!pip install folium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiChRVzN2LEa",
        "outputId": "13f93ca2-936e-4681-c2aa-3b0e7ccc3ca5"
      },
      "id": "OiChRVzN2LEa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lemminflect in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lemminflect) (1.23.5)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from folium) (0.7.0)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from folium) (3.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from folium) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from folium) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9->folium) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->folium) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "from spacy import displacy\n",
        "from collections import OrderedDict\n",
        "from lemminflect import getLemma, getInflection\n",
        "import requests\n",
        "import re\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import folium\n",
        "from IPython.display import IFrame"
      ],
      "metadata": {
        "id": "h0uMin1_02cV"
      },
      "id": "h0uMin1_02cV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global spaCy model\n",
        "NLP_MODEL = spacy.load('en_core_web_sm', exclude=['parser'])"
      ],
      "metadata": {
        "id": "WdFtwAopz-kg"
      },
      "id": "WdFtwAopz-kg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Entity Color Mapping\n"
      ],
      "metadata": {
        "id": "ICXaolz7p1V6"
      },
      "id": "ICXaolz7p1V6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Defines colors for different entity types, including a specific color for plant names (`PLANT`)."
      ],
      "metadata": {
        "id": "dbiS4iwJp5JI"
      },
      "id": "dbiS4iwJp5JI"
    },
    {
      "cell_type": "code",
      "source": [
        "BG_COLOR = {\n",
        "    'PLANT': '#a9dfbf',  ### the added plant name\n",
        "    'PLNAME':'#feca74',\n",
        "\t'GEONOUN': '#9cc9cc',\n",
        "\t'GPE':'#feca74',\n",
        "\t'CARDINAL':'#e4e7d2',\n",
        "\t'FAC':'#9cc9cc',\n",
        "\t'QUANTITY':'#e4e7d2',\n",
        "\t'PERSON':'#aa9cfc',\n",
        "\t'ORDINAL':'#e4e7d2',\n",
        "\t'ORG':'#7aecec',\n",
        "\t'NORP':'#d9fe74',\n",
        "\t'LOC':'#9ac9f5',\n",
        "\t'DATE':'#c7f5a9',\n",
        "\t'DISTANCE':'#edf5a9',\n",
        "\t'EVENT': '#e1a9f5',\n",
        "\t'TIME':'#a9f5bc',\n",
        "\t'WORK_OF_ART':'#e6c1d7',\n",
        "\t'LAW':'#e6e6c1',\n",
        "\t'LOCADV':'##f79188',\n",
        "\t'SP-PREP':'#f5b5cf',\n",
        "\t'PERCENT':'#c9ebf5',\n",
        "\t'MONEY':'#b3d6f2',\n",
        "\t'+EMOTION':'#94f72a',\n",
        "\t'-EMOTION':'#f75252',\n",
        "\t'TIME-SEM':'#d0e0f2',\n",
        "\t'MOVEMENT':'#f2d0d0',\n",
        "\t'no_tag':'#FFFFFF'\n",
        "}"
      ],
      "metadata": {
        "id": "-D9DgtU1p6kl"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-D9DgtU1p6kl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec6777f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ec6777f",
        "outputId": "dbfa8f87-161c-4618-e64a-83d461ada87a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/language.py:1113: RuntimeWarning: coroutine 'NamedEntityExtractor.process_text' was never awaited\n",
            "  return self.tokenizer(text)\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/language.py:1113: RuntimeWarning: coroutine 'main' was never awaited\n",
            "  return self.tokenizer(text)\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_label: GEONOUN\n",
            "base_label: GPE\n",
            "base_label: GPE\n",
            "Map has been saved to map.html\n"
          ]
        }
      ],
      "source": [
        "class NamedEntityExtractor:\n",
        "    def __init__(self, nlp_model=\"en_core_web_sm\"):\n",
        "        self.nlp = spacy.load(nlp_model, exclude=['parser'])\n",
        "        self.nlp.add_pipe(\"sentencizer\")\n",
        "        self.ruler = self.nlp.add_pipe(\"entity_ruler\", before='ner')\n",
        "        self.setup_entity_patterns()\n",
        "        self.combine = lambda x, y: (x[0], x[1], x[2]+' '+y[2], x[3])\n",
        "        self.geolocation_tags = ['GEO', 'PLNAME', 'GPE']  # Tags for which to perform geocoding\n",
        "        self.geocode_cache = {}\n",
        "\n",
        "\n",
        "    def setup_entity_patterns(self):\n",
        "        # Get the list of placenames and geonouns\n",
        "        place_names = [name.strip().title().replace(\"'S\", \"'s\") for name in open('resources/LD_placenames.txt').readlines()] #read and convert to title case\n",
        "        place_names += [name.upper() for name in place_names] #retain the upper case versions\n",
        "        geonouns = self.get_inflections([noun.strip() for noun in open('resources/geo_feature_nouns.txt').readlines()])\n",
        "\n",
        "        # Get the locative adverbs\n",
        "        loc_advs = [l.split()[0] for l in open('resources/locative_adverbs.txt').readlines()]\n",
        "        sp_prep  = [l.strip() for l in open('resources/spatial_prepositions.txt').readlines()\n",
        "                                                                    if len(l.strip())>2]\n",
        "        # Get distances\n",
        "        distances = [l.strip() for l in open('resources/distances.txt').readlines()]\n",
        "\n",
        "        # Get dates\n",
        "        dates     = [l.strip() for l in open('resources/dates.txt').readlines()]\n",
        "\n",
        "        # Get times\n",
        "        times     = [l.strip() for l in open('resources/times.txt').readlines()]\n",
        "\n",
        "        # Get events\n",
        "        events    = [l.strip() for l in open('resources/events.txt').readlines()]\n",
        "\n",
        "        # Get Plant- names this is new adding the plant names list to the NES\n",
        "        pnames = [l.strip() for l in open('resources/Plant_list.txt').readlines()]\n",
        "\n",
        "        # Get the list of positive and negative words from the sentiment lexicon\n",
        "        pos_words = [w.strip() for w in open('resources/positive-words.txt','r', encoding='latin-1').readlines()[35:]]\n",
        "        neg_words = [w.strip() for w in open('resources/negative-words.txt','r', encoding='latin-1').readlines()[35:]]\n",
        "\n",
        "\n",
        "        # Define the patterns for the EntityRuler by labelling all the names with the tag PLNAME\n",
        "        patterns = [{\"label\": \"PLANT\", \"pattern\": word} for word in pnames]\n",
        "        patterns +=  [{\"label\": \"PLNAME\",  \"pattern\": plname} for plname in set(place_names)]\n",
        "        patterns += [{\"label\": \"GEONOUN\", \"pattern\": noun} for noun in geonouns]\n",
        "        patterns += [{\"label\": \"+EMOTION\", \"pattern\": word} for word in pos_words]\n",
        "        patterns += [{\"label\": \"-EMOTION\", \"pattern\": word} for word in neg_words]\n",
        "        patterns += [{\"label\": \"EVENT\",   \"pattern\": word} for word in events]\n",
        "        patterns += [{\"label\": \"DATE\", \"pattern\": word} for word in dates]\n",
        "        patterns += [{\"label\": \"TIME\", \"pattern\": word} for word in times]\n",
        "        patterns += [{\"label\": \"DISTANCE\", \"pattern\": word} for word in distances]\n",
        "        patterns += [{\"label\": \"LOCADV\", \"pattern\": word} for word in loc_advs]\n",
        "        patterns += [{\"label\": \"SP-PREP\", \"pattern\": word} for word in sp_prep]\n",
        "\n",
        "\n",
        "        self.ruler.add_patterns(patterns)\n",
        "\n",
        "\n",
        "\n",
        "    # Get inflections and lemmas of geo nouns\n",
        "    def get_inflections(self,names_list):\n",
        "        gf_names_inflected = []\n",
        "        for w in names_list:\n",
        "            gf_names_inflected.append(w)\n",
        "            gf_names_inflected.extend(list(getInflection(w.strip(), tag='NNS', inflect_oov=False)))\n",
        "            gf_names_inflected.extend(list(getLemma(w.strip(), 'NOUN', lemmatize_oov=False)))\n",
        "        return list(set(gf_names_inflected))\n",
        "\n",
        "    # Generates a dictionary of entities with the indexes as keys\n",
        "    def extract_entities(self,text, ent_list, tag='PLNAME'):\n",
        "        sorted(set(ent_list), key=lambda x:len(x), reverse=True)\n",
        "        extracted_entities = {}\n",
        "        for ent in ent_list:\n",
        "            for match in re.finditer(f' {ent}[\\.,\\s\\n;:]', text):\n",
        "\n",
        "                extracted_entities[match.start()+1]=text[match.start()+1:match.end()-1], tag\n",
        "        return {i:extracted_entities[i] for i in sorted(extracted_entities.keys())}\n",
        "\n",
        "\n",
        "    def combine_multi_tokens(self,a_list):\n",
        "        new_list = [a_list.pop()]\n",
        "        while a_list:\n",
        "            last = a_list.pop()\n",
        "            if new_list[-1][0] - last[0] == 1:\n",
        "                new_list.append(self.combine(last, new_list.pop()))\n",
        "            else:\n",
        "                new_list.append(last)\n",
        "        return sorted(new_list)\n",
        "\n",
        "     # Generates a dictionary of semantic entities combining adjacent ones\n",
        "    def extract_sem_entities(self,processed_text, tag_types):\n",
        "        entities, tokens = {}, [token.text for token in processed_text]\n",
        "        for tag_type in tag_types:\n",
        "            tag_indices = [(i, token.idx, token.text, tag_type) for i, token in enumerate(processed_text)\n",
        "                                if token._.pymusas_tags[0].startswith(tag_type[0])]\n",
        "            if tag_indices:\n",
        "                for i, idx, token, tag in self.combine_multi_tokens(tag_indices):\n",
        "                    entities[idx] = token, tag\n",
        "        return OrderedDict(sorted(entities.items()))\n",
        "\n",
        "\n",
        "\n",
        "    # Generates a list of all tokens, tagged and untagged, for visualisation\n",
        "    def get_tagged_list(text, entities):\n",
        "        begin, tokens_tags = 0, []\n",
        "        for start, (ent, tag) in entities.items():\n",
        "            if begin <= start:\n",
        "                tokens_tags.append((text[begin:start], None))\n",
        "                tokens_tags.append((text[start:start+len(ent)], tag))\n",
        "                begin = start+len(ent)\n",
        "        tokens_tags.append((text[begin:], None)) #add the last untagged chunk\n",
        "        return tokens_tags\n",
        "\n",
        "    def merge_entities(self, doc):\n",
        "        merged_entities = []\n",
        "        temp_entity = {\"text\": \"\", \"start\": None, \"end\": None, \"label\": None}\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if temp_entity[\"label\"] == ent.label_ and (temp_entity[\"end\"] == ent.start_char or temp_entity[\"end\"] + 1 == ent.start_char):\n",
        "                temp_entity[\"text\"] += \" \" + ent.text\n",
        "                temp_entity[\"end\"] = ent.end_char\n",
        "            else:\n",
        "                if temp_entity[\"text\"]:\n",
        "                    merged_entities.append(temp_entity.copy())\n",
        "                temp_entity = {\"text\": ent.text, \"start\": ent.start_char, \"end\": ent.end_char, \"label\": ent.label_}\n",
        "\n",
        "        if temp_entity[\"text\"]:\n",
        "            merged_entities.append(temp_entity.copy())\n",
        "\n",
        "        return merged_entities\n",
        "\n",
        "\n",
        "\n",
        "    async def geocode(self, place_name):\n",
        "        if place_name in self.geocode_cache:\n",
        "            return self.geocode_cache[place_name]\n",
        "\n",
        "        base_url = \"https://nominatim.openstreetmap.org/search\"\n",
        "        params = {'q': place_name, 'format': 'json'}\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.get(base_url, params=params) as response:\n",
        "                if response.status == 200:\n",
        "                    data = await response.json()\n",
        "                    if data:\n",
        "                        result = {'latitude': data[0].get('lat'), 'longitude': data[0].get('lon')}\n",
        "                        self.geocode_cache[place_name] = result\n",
        "                        return result\n",
        "        return {'latitude': None, 'longitude': None}\n",
        "\n",
        "    async def convert_to_iob_format(self, merged_entities, doc):\n",
        "        iob_entities = []\n",
        "        for sent in doc.sents:\n",
        "            sent_entities = [e for e in merged_entities if e[\"start\"] >= sent.start_char and e[\"end\"] <= sent.end_char]\n",
        "            for token in sent:\n",
        "                merged_entity = next((e for e in sent_entities if e[\"start\"] <= token.idx < e[\"end\"]), None)\n",
        "                if merged_entity:\n",
        "                    tag_prefix = 'B-' if token.idx == merged_entity[\"start\"] else 'I-'\n",
        "                    base_label = merged_entity[\"label\"].split('-')[-1]\n",
        "                    print('base_label:',base_label )\n",
        "                    if base_label in [\"PLNAME\", \"GEONOUN\",  \"GPE\"]:\n",
        "                        geolocation = await self.geocode(merged_entity[\"text\"])\n",
        "\n",
        "                    else:\n",
        "                        geolocation = None\n",
        "                    iob_entities.append((token.text, tag_prefix + merged_entity[\"label\"], geolocation))\n",
        "                else:\n",
        "                    iob_entities.append((token.text, 'O', None))\n",
        "        return iob_entities\n",
        "\n",
        "\n",
        "    async def process_text(self, text):\n",
        "        doc = self.nlp(text)\n",
        "        merged_entities = self.merge_entities(doc)\n",
        "        return await self.convert_to_iob_format(merged_entities, doc)\n",
        "\n",
        "\n",
        "    def visualize_entities(self, text):\n",
        "        doc = self.nlp(text)\n",
        "        options = {\"ents\": list(BG_COLOR.keys()), \"colors\": BG_COLOR}\n",
        "        displacy.render(doc, style=\"ent\", jupyter=True, options=options)\n",
        "\n",
        "    def visualize_on_map(self, entities):\n",
        "        # Create a map centered around a default location\n",
        "        map_center = [30, 0]  #  center of the world\n",
        "        map = folium.Map(location=map_center, zoom_start=2)\n",
        "\n",
        "        # Add markers for each geolocated entity\n",
        "        for _, label, geo in entities:\n",
        "            if geo and geo['latitude'] and geo['longitude']:\n",
        "                folium.Marker(\n",
        "                    location=[float(geo['latitude']), float(geo['longitude'])],\n",
        "                    popup=label,\n",
        "                    icon=folium.Icon(icon=\"info-sign\")\n",
        "                ).add_to(map)\n",
        "\n",
        "        return map\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    text = \"The Nile is a major north-flowing river in Northeastern Africa.\"\n",
        "    extractor = NamedEntityExtractor()\n",
        "    entities = await extractor.process_text(text)\n",
        "    print(entities)\n",
        "    extractor.visualize_entities(text)\n",
        "    # Assuming visualize_on_map returns a folium.Map object\n",
        "    map_obj = extractor.visualize_on_map(entities)\n",
        "\n",
        "    # Save the map to an HTML file\n",
        "    map_obj.save(\"map.html\")\n",
        "    print(\"Map has been saved to map.html\")\n",
        "    return map_obj\n",
        "\n",
        "# Ensure asyncio.run is called in the main guard to prevent running the async function on import\n",
        "if __name__ == \"__main__\":\n",
        "    # Directly await the main function in a notebook cell\n",
        "  map_obj = await main()\n",
        "\n",
        "# Display the map\n",
        "map_obj\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "PEoljBHiCbup",
        "outputId": "c7c52ce3-1150-44eb-a9d2-0b583b39da72"
      },
      "id": "PEoljBHiCbup",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_label: GEONOUN\n",
            "base_label: GPE\n",
            "base_label: GPE\n",
            "[('The', 'O', None), ('Nile', 'O', None), ('is', 'O', None), ('a', 'O', None), ('major', 'O', None), ('north', 'O', None), ('-', 'O', None), ('flowing', 'O', None), ('river', 'B-GEONOUN', {'latitude': '51.1410065', 'longitude': '1.2748110678641456'}), ('in', 'O', None), ('Northeastern', 'B-GPE', {'latitude': '46.2588615', 'longitude': '-83.6403313'}), ('Africa', 'I-GPE', {'latitude': '46.2588615', 'longitude': '-83.6403313'}), ('.', 'O', None)]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The Nile is a major north-flowing \n",
              "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    river\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GEONOUN</span>\n",
              "</mark>\n",
              " in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Northeastern Africa\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map has been saved to map.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<folium.folium.Map at 0x7b15f6fb2260>"
            ],
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_5e96332347158becf095b4269c9f7e9d {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_5e96332347158becf095b4269c9f7e9d&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_5e96332347158becf095b4269c9f7e9d = L.map(\n",
              "                &quot;map_5e96332347158becf095b4269c9f7e9d&quot;,\n",
              "                {\n",
              "                    center: [30.0, 0.0],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    zoom: 2,\n",
              "                    zoomControl: true,\n",
              "                    preferCanvas: false,\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_ecfb6f0157f78df37ad876150a229b7b = L.tileLayer(\n",
              "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
              "            ).addTo(map_5e96332347158becf095b4269c9f7e9d);\n",
              "        \n",
              "    \n",
              "            var marker_dac497cc9dafaba7d378901d12cc9c9f = L.marker(\n",
              "                [51.1410065, 1.2748110678641456],\n",
              "                {}\n",
              "            ).addTo(map_5e96332347158becf095b4269c9f7e9d);\n",
              "        \n",
              "    \n",
              "            var icon_a7565a76f18193800e1a7a564c9b5c4e = L.AwesomeMarkers.icon(\n",
              "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
              "            );\n",
              "            marker_dac497cc9dafaba7d378901d12cc9c9f.setIcon(icon_a7565a76f18193800e1a7a564c9b5c4e);\n",
              "        \n",
              "    \n",
              "        var popup_79f318df175b4bc862ce4215b6a5c4c2 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
              "\n",
              "        \n",
              "            \n",
              "                var html_22547aa83f5d185fdbfe319a874218f4 = $(`&lt;div id=&quot;html_22547aa83f5d185fdbfe319a874218f4&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;B-GEONOUN&lt;/div&gt;`)[0];\n",
              "                popup_79f318df175b4bc862ce4215b6a5c4c2.setContent(html_22547aa83f5d185fdbfe319a874218f4);\n",
              "            \n",
              "        \n",
              "\n",
              "        marker_dac497cc9dafaba7d378901d12cc9c9f.bindPopup(popup_79f318df175b4bc862ce4215b6a5c4c2)\n",
              "        ;\n",
              "\n",
              "        \n",
              "    \n",
              "    \n",
              "            var marker_d397f291bea88bd830e788ea86bf0577 = L.marker(\n",
              "                [46.2588615, -83.6403313],\n",
              "                {}\n",
              "            ).addTo(map_5e96332347158becf095b4269c9f7e9d);\n",
              "        \n",
              "    \n",
              "            var icon_2881429777e8971c6f81330c8d986c44 = L.AwesomeMarkers.icon(\n",
              "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
              "            );\n",
              "            marker_d397f291bea88bd830e788ea86bf0577.setIcon(icon_2881429777e8971c6f81330c8d986c44);\n",
              "        \n",
              "    \n",
              "        var popup_f251a48489d8407d53fcf38a1067e612 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
              "\n",
              "        \n",
              "            \n",
              "                var html_17d1594da63ab2ce2743c56bc9a11b5c = $(`&lt;div id=&quot;html_17d1594da63ab2ce2743c56bc9a11b5c&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;B-GPE&lt;/div&gt;`)[0];\n",
              "                popup_f251a48489d8407d53fcf38a1067e612.setContent(html_17d1594da63ab2ce2743c56bc9a11b5c);\n",
              "            \n",
              "        \n",
              "\n",
              "        marker_d397f291bea88bd830e788ea86bf0577.bindPopup(popup_f251a48489d8407d53fcf38a1067e612)\n",
              "        ;\n",
              "\n",
              "        \n",
              "    \n",
              "    \n",
              "            var marker_40c594db2bc8c73f1aafad33e9541067 = L.marker(\n",
              "                [46.2588615, -83.6403313],\n",
              "                {}\n",
              "            ).addTo(map_5e96332347158becf095b4269c9f7e9d);\n",
              "        \n",
              "    \n",
              "            var icon_de6c4423ac358114b76a0356f9bca508 = L.AwesomeMarkers.icon(\n",
              "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
              "            );\n",
              "            marker_40c594db2bc8c73f1aafad33e9541067.setIcon(icon_de6c4423ac358114b76a0356f9bca508);\n",
              "        \n",
              "    \n",
              "        var popup_4c30b55616c75dd9a348cf30529ada45 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
              "\n",
              "        \n",
              "            \n",
              "                var html_1b1613be62975d5a93fccf44f0fa3b7a = $(`&lt;div id=&quot;html_1b1613be62975d5a93fccf44f0fa3b7a&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;I-GPE&lt;/div&gt;`)[0];\n",
              "                popup_4c30b55616c75dd9a348cf30529ada45.setContent(html_1b1613be62975d5a93fccf44f0fa3b7a);\n",
              "            \n",
              "        \n",
              "\n",
              "        marker_40c594db2bc8c73f1aafad33e9541067.bindPopup(popup_4c30b55616c75dd9a348cf30529ada45)\n",
              "        ;\n",
              "\n",
              "        \n",
              "    \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Dyp0SHN_GEw"
      },
      "id": "4Dyp0SHN_GEw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}